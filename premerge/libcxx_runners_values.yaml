githubConfigUrl: "https://github.com/llvm"
githubConfigSecret: "github-token"

minRunners: 0
maxRunners: 32

runnerGroup: ${ runner_group_name }

template:
  metadata:
    annotations:
      cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
  spec:
    tolerations:
    - key: "premerge-platform-libcxx"
      operator: "Equal"
      value: "linux-libcxx"
      effect: "NoSchedule"
    nodeSelector:
      premerge-platform-libcxx: linux-libcxx
    containers:
    - name: runner
      image: ${ runner_image }
      command: ["${ command }"]
      resources:
        # If we don't set the CPU request high-enough here, 2 runners might
        # be scheduled on the same pod, meaning 2 jobs, and they will starve
        # each other.
        #
        # This number should be:
        #  - greater than number_of_cores / 2:
        #    A value lower than that could allow the scheduler to put 2
        #    runners on the same node. Meaning 2 jobs sharing the resources of
        #    a single node.
        #  - lower than number_of_cores:
        #    Each pod has some basic services running (metrics for ex). Those
        #    already require some amount of CPU (~0.5). This means we don't
        #    exactly have N cores to allocate, but N - epsilon.
        #
        # We also need to request sufficient memory to not get OOM killed.
        requests:
          cpu: 28
          memory: "100Gi"
        limits:
          cpu: 32
          memory: "128Gi"

